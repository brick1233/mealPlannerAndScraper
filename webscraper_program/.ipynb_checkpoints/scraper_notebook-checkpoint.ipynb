{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "821f3c5b",
   "metadata": {},
   "source": [
    "### Build web scraper\n",
    "- functions to:\n",
    "    - find the website domain ie) verify the html structure\n",
    "    - extract data from html using different tag/attribute combos\n",
    "    - parse the html and return the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "480b1ca8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'span'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  Cell \u001b[0;32mIn[3], line 18\u001b[0m\n    x = [item for item in li_tags.span.decompose()]\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\element.py:2428\u001b[1;36m in \u001b[1;35m__getattr__\u001b[1;36m\n\u001b[1;33m    raise AttributeError(\u001b[1;36m\n",
      "\u001b[1;31mAttributeError\u001b[0m\u001b[1;31m:\u001b[0m ResultSet object has no attribute 'span'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <h1>Hello, BeautifulSoup!</h1>\n",
    "        <ul>\n",
    "            <li><a href=\"http://example.com\">Example.com <span>(Link)</span></a></li>\n",
    "            <li><a href=\"http://scrapy.org\">Scrapy.com <span>(Link)</span></a></li>\n",
    "        </ul>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "li_tags = soup.find_all('li')\n",
    "\n",
    "for li in li_tags:\n",
    "    li.span.decompose()\n",
    "\n",
    "print(soup)\n",
    "\n",
    "def decomposeSpan(htmlTags):\n",
    "    for tags in htmlTags:\n",
    "        tags.span.decompose()\n",
    "    \n",
    "\n",
    "def find_findAll(soup, key):\n",
    "    \n",
    "    # find the section\n",
    "    section_container = soup.find(key[0], key[1])\n",
    "    \n",
    "    # If the container is found, find all list items within it and return\n",
    "    if section_container:\n",
    "        filteredContainer = section_container.find_all(key[2], key[3])\n",
    "        if len(key) > 4:\n",
    "            filteredContainer = key[4](filteredContainer)\n",
    "            \n",
    "        data = \"\\n\".join([item.text.strip()\n",
    "                         for item in filteredContainer])\n",
    "        \n",
    "        return data\n",
    "    else:\n",
    "        return f\"Error with these tags:\\n{key}\\nAnd this soup:\\n{soup}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9a5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime as date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5284e03",
   "metadata": {},
   "source": [
    "##### Function to find domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcf2db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find the domain of a url\n",
    "def parseDomain(url):\n",
    "    # match for \"www.MATCH.com\"\n",
    "    findDomain = re.search(r\"\\.[A-Za-z0-9]+\\.\", url)\n",
    "\n",
    "    # Access the matched text using group(), else report bad url\n",
    "    if findDomain:\n",
    "        return(findDomain.group()[1:-1])\n",
    "    else:\n",
    "        return(\"bad_url\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1242364a",
   "metadata": {},
   "source": [
    "##### Function to parse html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e06943b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'findAll' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 18\u001b[1;36m\n\u001b[1;33m    allRecipe_ingredient = (findAll,('li', {'class':'mntl-structured-ingredients__list-item'}))\u001b[1;36m\n",
      "\u001b[1;31mNameError\u001b[0m\u001b[1;31m:\u001b[0m name 'findAll' is not defined\n"
     ]
    }
   ],
   "source": [
    "#function to parse the ingredients and recipe contained\n",
    "#by a website. htmlKey is a dictionary containing the relevant html tags etc.\n",
    "def parseRecipeSite(url, htmlKey):\n",
    "    #send request and read html if good request\n",
    "    page = requests.get(url) #read page\n",
    "    if page.status_code == 200:\n",
    "        #get the soup\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        #isolate the text from the website and return\n",
    "        ingredientStr = htmlKey[0][0](soup, htmlKey[0][1])\n",
    "        recipeStr =  htmlKey[1][0](soup, htmlKey[1][1])\n",
    "        return (ingredientStr, recipeStr)\n",
    "    else:\n",
    "        return f'This url \"{url}\" had a bad request. Error #: {page.status_code}'\n",
    "    \n",
    "#allrecipes key\n",
    "allRecipe_ingredient = (findAll,('li', {'class':'mntl-structured-ingredients__list-item'}))\n",
    "allRecipe_recipe = (findAll,(\"li\", {\"class\" :\"comp mntl-sc-block mntl-sc-block-startgroup mntl-sc-block-group--LI\"}))\n",
    "#allRecipe_recipe = (find_findAll,(\"li\", {\"class\" :\"comp mntl-sc-block mntl-sc-block-startgroup mntl-sc-block-group--LI\"}, \"p\", {'class':\"comp mntl-sc-block mntl-sc-block-html\"}))\n",
    "\n",
    "htmlKeys = {\"allrecipes\":(allRecipe_ingredient,allRecipe_recipe), \n",
    "            \"afamilyfeast\": (familyFeast_ingredient,familyFeast_recipe),\n",
    "            \"bad_url\": \"\"}\n",
    "\n",
    "url1= \"https://www.allrecipes.com/recipe/221333/maja-blanca/\"\n",
    "url2 = \"https://www.allrecipes.com/recipe/26472/the-best-chicken-soup-ever/\"\n",
    "\n",
    "x = parseRecipeSite(url1, htmlKeys['allrecipes'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4283e701",
   "metadata": {},
   "source": [
    "##### Functions to extract data from html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec84c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#function to find all, provide soup and relvant tags\n",
    "def findAll(soup, key):\n",
    "    #isolate the section and run if exists\n",
    "    htmlList = soup.find_all(key[0], key[1])\n",
    "    print(htmlList)\n",
    "    print(\"\\n\")\n",
    "    if htmlList:\n",
    "        data = str()\n",
    "        #add each item to the string and return\n",
    "        for item in htmlList:\n",
    "            # Get text from each tag\n",
    "            data += item.text.strip() + \"\\n\"\n",
    "        return data\n",
    "    else:\n",
    "        return f\"Error with these tags:\\n{key}\\nAnd this soup:\\n{soup}\"\n",
    "\n",
    "#function to find a specific section, then read the text of a certain tag\n",
    "def find_findAll(soup, key):\n",
    "    #find the section\n",
    "    section_container = soup.find(key[0], key[1])\n",
    "    print(section_container)\n",
    "    print(\"\\n\")\n",
    "    # If the container is found, find all list items within it and return\n",
    "    if section_container:\n",
    "        data = \"\\n\".join([item.text.strip() for item in section_container.find_all(key[2], key[3])])\n",
    "        \n",
    "        return data\n",
    "    else:\n",
    "        return f\"Error with these tags:\\n{key}\\nAnd this soup:\\n{soup}\"\n",
    "    \n",
    "def testUrl(url, key):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "   \n",
    "    htmlList = soup.find_all(key[0], key[1])\n",
    "    print(htmlList)\n",
    "    print(\"\\n\")\n",
    "    if htmlList:\n",
    "        data = str()\n",
    "        #add each item to the string and return\n",
    "        for item in htmlList:\n",
    "            # Get text from each tag\n",
    "            data += item.text.strip() + \"\\n\"\n",
    "        return data\n",
    "    else:\n",
    "        return f\"Error with these tags:\\n{key}\\nAnd this soup:\\n{soup}\"\n",
    "    \n",
    "familyFeast_ingredient = (find_findAll, ('div', {\n",
    "                          'class': \"tasty-recipes-ingredients-body\"}, 'p', {\"data-tr-ingredient-checkbox\": \"\"}))\n",
    "familyFeast_recipe = (\n",
    "    findAll, ('li', {'id': re.compile(r'instruction-step-*')}))\n",
    "\n",
    "find = (re.compile(r\"[A-z]*\"), {'data-tr-ingredient-checkbox': \"\"})\n",
    "\n",
    "url = \"https://www.afamilyfeast.com/strawberry-torte/\"\n",
    "\n",
    "print(testUrl(url, find))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf8c0ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html>\n",
      "<body>\n",
      "<h1>Hello, BeautifulSoup!</h1>\n",
      "<ul>\n",
      "<li><a href=\"http://example.com\">Example.com </a></li>\n",
      "<li><a href=\"http://scrapy.org\">Scrapy.com </a></li>\n",
      "</ul>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <h1>Hello, BeautifulSoup!</h1>\n",
    "        <ul>\n",
    "            <li><a href=\"http://example.com\">Example.com <span>(Link)</span></a></li>\n",
    "            <li><a href=\"http://scrapy.org\">Scrapy.com <span>(Link)</span></a></li>\n",
    "        </ul>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "li_tags = soup.find_all('li')\n",
    "li_tags = [item.span.decompose() for item in li_tags]\n",
    "\n",
    "print(soup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97d7bf0",
   "metadata": {},
   "source": [
    "##### Function to store scraped recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71aed773",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def storeScrapedRecipes (file, recipeTuple):\n",
    "\n",
    "    recipeName = file.name.split(\"/\")[-1].split(\".\")[0]\n",
    "    ingredients = re.sub(r\"\\n+\", \"\\n\", recipeTuple[0])\n",
    "    \n",
    "    recipe = re.sub(r\"\\n+\", \"\\n\", recipeTuple[1])\n",
    "    string = \"\"\n",
    "    string += f'Name:\\n{recipeName.replace(\"-\", \" \")}\\n\\n'\n",
    "    string += f'Ingredients:\\n{ingredients}\\n\\n'\n",
    "    string += f'Directions:\\n{recipe}\\n\\n'\n",
    "    string += f'Notes:\\nScraped from a website on {str(date.today()).split(\" \")[0]}\\n\\n'\n",
    "    string += f'Date:\\n{str(date.today()).split(\" \")[0]}'\n",
    "    #file.write(re.sub(\"\\n\",\"\\n\",string))\n",
    "    file.write(string)\n",
    "\n",
    "#with open(f'{newDir}/{\"https://www.allrecipes.com/recipe/26472/the-best-chicken-soup-ever/\".split(\"/\")[-2]}.txt', \"w+\", encoding=\"utf-8\") as file:\n",
    "            \n",
    "            #storeScrapedRecipes(file, (\"testIngre\", \"TestReci\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e383f",
   "metadata": {},
   "source": [
    "##### get urls from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cccbbd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findRecipeUrls(scrapeFilePath):\n",
    "    try:\n",
    "        with open(scrapeFilePath, \"r\") as file:\n",
    "            # Read lines, strip whitespace, and store in a list\n",
    "            urls = [line.strip() for line in file.readlines()]\n",
    "            # Convert the list to a tuple\n",
    "            return tuple(urls)\n",
    "    except:\n",
    "        print(f\"Error: File '{scrapeFilePath}' not found.\")\n",
    "        return tuple()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3183906",
   "metadata": {},
   "source": [
    "##### Main Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c83c90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  Cell \u001b[0;32mIn[7], line 37\u001b[0m\n    ingredients, recipe = parseRecipeSite(url, htmlKeys[domain])\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[3], line 5\u001b[0m in \u001b[0;35mparseRecipeSite\u001b[0m\n    page = requests.get(url) #read page\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m in \u001b[0;35mget\u001b[0m\n    return request(\"get\", url, params=params, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m in \u001b[0;35mrequest\u001b[0m\n    return session.request(method=method, url=url, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m in \u001b[0;35mrequest\u001b[0m\n    resp = self.send(prep, **send_kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:747\u001b[0m in \u001b[0;35msend\u001b[0m\n    r.content\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:899\u001b[0m in \u001b[0;35mcontent\u001b[0m\n    self._content = b\"\".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b\"\"\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:816\u001b[0m in \u001b[0;35mgenerate\u001b[0m\n    yield from self.raw.stream(chunk_size, decode_content=True)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:624\u001b[0m in \u001b[0;35mstream\u001b[0m\n    for line in self.read_chunked(amt, decode_content=decode_content):\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:832\u001b[0m in \u001b[0;35mread_chunked\u001b[0m\n    decoded = self._decode(\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:407\u001b[0m in \u001b[0;35m_decode\u001b[0m\n    data = self._decoder.decompress(data)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\brotli\\brotli.py:407\u001b[1;36m in \u001b[1;35mdecompress\u001b[1;36m\n\u001b[1;33m    rc = lib.BrotliDecoderDecompressStream(self._decoder,\u001b[1;36m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#set-up html parsing keys\n",
    "#allrecipes key\n",
    "allRecipe_ingredient = (findAll,('li', {'class':'mntl-structured-ingredients__list-item'}))\n",
    "allRecipe_recipe = (findAll,(\"li\", {\"class\" :\"comp mntl-sc-block mntl-sc-block-startgroup mntl-sc-block-group--LI\"}))\n",
    "\n",
    "#afamilyfeast key\n",
    "familyFeast_ingredient = (find_findAll, ('div', {'class':\"tasty-recipes-ingredients-body\"}, 'li'))\n",
    "familyFeast_recipe = (findAll, ('li', {'id':re.compile(r'instruction-step-*')}))\n",
    "\n",
    "#compile keys in a dictionary of nested tuples\n",
    "htmlKeys = {\"allrecipes\":(allRecipe_ingredient,allRecipe_recipe), \n",
    "            \"afamilyfeast\": (familyFeast_ingredient,familyFeast_recipe),\n",
    "            \"bad_url\": \"\"}\n",
    "\n",
    "#set up folder to house the scraped recipes\n",
    "wd = \n",
    "scrapedDir = os.path.join(wd, \"scrapedRecipes\")\n",
    "newDir = f'{scrapedDir}/{str(date.today()).split(\" \")[0]}'\n",
    "if os.path.isdir(newDir):\n",
    "    shutil.rmtree(newDir)\n",
    "\n",
    "os.mkdir(newDir)\n",
    "\n",
    "#define web pages of interest\n",
    "#url = 'https://www.allrecipes.com/recipe/26472/the-best-chicken-soup-ever/\\nhttps://www.afamilyfeast.com/strawberry-torte/' \n",
    "#urlList = url.split(\"\\n\")\n",
    "\n",
    "urlTuple = findRecipeUrls(f'{scrapedDir}/recipeURLs.txt')\n",
    "#print(urlTuple)\n",
    "\n",
    "for url in urlTuple:\n",
    "    domain = parseDomain(url)\n",
    "    if domain == \"bad_url\":\n",
    "        htmlKeys[\"bad_url\"] += url +\"\\n\"\n",
    "        print(\"BAD\")\n",
    "    else:\n",
    "        ingredients, recipe = parseRecipeSite(url, htmlKeys[domain])\n",
    "        #print(f'{newDir}/{url.split(\"/\")[-2]}')\n",
    "        with open(f'{newDir}/{url.split(\"/\")[-2]}.txt', \"w+\", encoding=\"utf-8\") as file:\n",
    "            print(\"here\")\n",
    "            storeScrapedRecipes(file, (ingredients, recipe))\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ec420",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2de870",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#set-up html parsing keys\n",
    "#allrecipes key\n",
    "allRecipe_ingredient = (findAll,('li', {'class':'mntl-structured-ingredients__list-item'}))\n",
    "allRecipe_recipe = (findAll,(\"li\", {\"class\" :\"comp mntl-sc-block mntl-sc-block-startgroup mntl-sc-block-group--LI\"}))\n",
    "\n",
    "#afamilyfeast key\n",
    "familyFeast_ingredient = (find_findAll, ('div', {'class':\"tasty-recipes-ingredients-body\"}, 'li'))\n",
    "familyFeast_recipe = (findAll, ('li', {'id':re.compile(r'instruction-step-*')}))\n",
    "\n",
    "#compile keys in a dictionary of nested tuples\n",
    "htmlKeys = {\"allrecipes\":(allRecipe_ingredient,allRecipe_recipe), \n",
    "            \"afamilyfeast\": (familyFeast_ingredient,familyFeast_recipe),\n",
    "            \"bad_url\": \"\"}\n",
    "\n",
    "#set up folder to house the scraped recipes\n",
    "wd = \n",
    "scrapedDir = os.path.join(wd, \"scrapedRecipes\")\n",
    "newDir = f'{scrapedDir}/{str(date.today()).split(\" \")[0]}'\n",
    "if os.path.isdir(newDir):\n",
    "    shutil.rmtree(newDir)\n",
    "\n",
    "os.mkdir(newDir)\n",
    "\n",
    "#define web pages of interest\n",
    "#url = 'https://www.allrecipes.com/recipe/26472/the-best-chicken-soup-ever/\\nhttps://www.afamilyfeast.com/strawberry-torte/' \n",
    "#urlList = url.split(\"\\n\")\n",
    "\n",
    "urlTuple = findRecipeUrls(f'{scrapedDir}/recipeURLs.txt')\n",
    "#print(urlTuple)\n",
    "\n",
    "for url in urlTuple:\n",
    "    domain = parseDomain(url)\n",
    "    if domain == \"bad_url\":\n",
    "        htmlKeys[\"bad_url\"] += url +\"\\n\"\n",
    "        print(\"BAD\")\n",
    "    else:\n",
    "        ingredients, recipe = parseRecipeSite(url, htmlKeys[domain])\n",
    "        #print(f'{newDir}/{url.split(\"/\")[-2]}')\n",
    "        with open(f'{newDir}/{url.split(\"/\")[-2]}.txt', \"w+\", encoding=\"utf-8\") as file:\n",
    "            print(\"here\")\n",
    "            storeScrapedRecipes(file, (ingredients, recipe))\n",
    "            \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
